{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4463fd30-1e66-4b69-8c63-5a6f983078c6",
   "metadata": {},
   "source": [
    "# Data Collection: Scrape the Met Website's Object Metadata & Images\n",
    "\n",
    "The Met dataset currently available on Kaggle (https://www.kaggle.com/datasets/metmuseum/the-metropolitan-museum-of-art-open-access) includes over 400K rows of data conducive to meaningful analysis and ML modeling.\n",
    "\n",
    "It also now offers an API for its collection data (https://metmuseum.github.io/), let's compare the data from scraping vs. that from the API. \n",
    "\n",
    "However, this data is not exactly the same as what is offered directly on its website, suggesting that the client-facing data is coming from a non-public API / backend. Since the Met's website contains the most complete and error-free information about its collection, the scraper created below is designed to acquire this data for comparison to its Kaggle and free API data.\n",
    "\n",
    "One important consideration when scraping any website is copyright regulations. The data from the Met's public API states that it is available for public use without fees, but scraping content directly from the Met website does not necessarily offer the same protections, as it also features images that still under copyright. \n",
    "\n",
    "Therefore, while data scraped from the Met website can offer a wealth of insights, the scraper below is intended for educational purposes only, not for commercial use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4e1d162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24ce680b-0682-4bcd-8f51-96a86c3c0ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scrape HTML from URL based on selectors\n",
    "\n",
    "async def get_html(url, *selectors, sleep=5, retries=3):\n",
    "    html = None\n",
    "    \n",
    "    # Allow 3 retries\n",
    "    for i in range(1, retries+1):\n",
    "        \n",
    "        # Sleep between tries to avoid overloading server\n",
    "        time.sleep(sleep * i)\n",
    "        \n",
    "        try:\n",
    "            async with async_playwright() as p:\n",
    "                browser = await p.firefox.launch()\n",
    "                page = await browser.new_page()\n",
    "                await page.goto(url)\n",
    "                \n",
    "                # Print page title as confirmation\n",
    "                title = await page.title()\n",
    "                print(title)\n",
    "                         \n",
    "                # Iterate over *selectors\n",
    "                # Concatenate scraped HTML\n",
    "                for s in selectors:\n",
    "                    html_next = await page.inner_html(s)\n",
    "                    html = '{0}\\n{1}'.format(html, html_next)\n",
    "        \n",
    "        # Catch timeout errors\n",
    "        except PlaywrightTimeout:\n",
    "            print(f\"Timeout error on {url}\")\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204618f5-03ab-4903-84b7-086736e8e689",
   "metadata": {},
   "source": [
    "### Add \\*args for selectors as necessary\n",
    "\n",
    "The use of \\*selector in the *get_html()* function represents a list of variables. It accepts as few or as many variables as entered whenever the function is called.\n",
    "\n",
    "This example uses two (2) selector arguments:\n",
    "* 1. '#artwork-section' -- top-level details (Title, Date, Nationality, 'On View') and image data\n",
    "* 2. '#details #overview' -- object details  (Acq. Date, Medium, Credit, Artist, Country)\n",
    "\n",
    "These selectors can be substituted for any combination and any number of combinations within reason (please respect the server you're scraping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ca04996-bd0b-4587-81e5-d6cff7ad0e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Handle scraping requests for each URL in list\n",
    "\n",
    "async def scrape_data(links, save_paths, DIR, *selectors):\n",
    "    \n",
    "    # Iterate over list of URLS\n",
    "    for i in range(len(links)):\n",
    "        \n",
    "        # Extract URL from list\n",
    "        url = links[i]\n",
    "        \n",
    "        # Generate save path from directory\n",
    "        save_path = os.path.join(DIR, save_paths[i])\n",
    "\n",
    "        if os.path.exists(save_path):\n",
    "            continue\n",
    "\n",
    "        # Get HTML from URL based on specified selectors\n",
    "        html = await get_html(url, *selectors)\n",
    "        \n",
    "        if not html:\n",
    "            continue\n",
    "\n",
    "        # Save scraped HTML to file\n",
    "        with open(save_path, \"w+\") as f:\n",
    "            f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1cdbabc-4e4a-4d54-9b5d-aa3f3c673520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main function\n",
    "\n",
    "async def main():\n",
    "    \n",
    "    # Create directories for the scraped data\n",
    "    OBJECTS_DIR = \"data/met/scrapes/objects\"\n",
    "    \n",
    "    # Import existing objects table to extract links and object ids\n",
    "    met_objects = pd.read_csv('met_objects.csv')\n",
    "    \n",
    "    # Create list of URLs to objects\n",
    "    met_links = met_objects['Link_Resource']\n",
    "\n",
    "    # Create list of HTML paths to save scraped HTML content (one path per object)\n",
    "    met_names = met_objects['Object_Name'].str.replace(\" \", \"-\").str.lower()\n",
    "    met_numbers = met_objects['Object_Number'].str.replace(\".\", \"-\")\n",
    "    met_paths = \"met_\" + met_names + \"_\" + met_numbers + \".html\"\n",
    "    \n",
    "    # Scrape data\n",
    "    await scrape_data(met_links, met_paths, OBJECTS_DIR, '#artwork-section', '#details #overview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54270743-d1ab-46a4-80bf-3a21813bc95e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run program\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d223c-f417-4cee-b218-e2a232ebe18c",
   "metadata": {},
   "source": [
    "## Test: Scrape a Single Page\n",
    "\n",
    "The test case below returns content from the function *await get_html(url, url_index, \\*selector)* for a single url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633f85d-2549-435e-8ab9-f815e4f14633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape test page\n",
    "\n",
    "# Create directories for the scraped data\n",
    "OBJECTS_DIR = \"data/met/scrapes/objects\"\n",
    "\n",
    "# Import existing objects table to extract links and object ids\n",
    "met_objects = pd.read_csv('met_objects.csv')\n",
    "\n",
    "# Create list of URLs to objects\n",
    "met_links = met_objects['Link_Resource']\n",
    "\n",
    "html = await get_html(met_links[1789], '#artwork-section', '#details #overview')\n",
    "\n",
    "with open(\"data/met/scrapes/objects/test.html\", \"w+\") as f:\n",
    "            f.write(html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
